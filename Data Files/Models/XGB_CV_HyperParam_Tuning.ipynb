{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMgj4uJQCBufdzg8x9+TIU/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MZiaAfzal71/Average_Weighted_Path_Vector/blob/main/Data%20Files/Models/XGB_CV_HyperParam_Tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M1vEziapD5j3",
        "outputId": "3c8549cb-ade5-436f-a6a0-c120985d85cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Average_Weighted_Path_Vector'...\n",
            "remote: Enumerating objects: 718, done.\u001b[K\n",
            "remote: Counting objects: 100% (133/133), done.\u001b[K\n",
            "remote: Compressing objects: 100% (123/123), done.\u001b[K\n",
            "remote: Total 718 (delta 84), reused 8 (delta 8), pack-reused 585 (from 1)\u001b[K\n",
            "Receiving objects: 100% (718/718), 30.79 MiB | 23.96 MiB/s, done.\n",
            "Resolving deltas: 100% (238/238), done.\n",
            "/content/Average_Weighted_Path_Vector/Data Files\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/MZiaAfzal71/Average_Weighted_Path_Vector.git\n",
        "%cd Average_Weighted_Path_Vector/Data\\ Files"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install osfclient\n",
        "import shutil\n",
        "from osfclient.api import OSF\n",
        "from subprocess import run\n",
        "import os\n",
        "\n",
        "# Replace with your OSF project ID\n",
        "project_id = \"p5ga2\"   # e.g. from https://osf.io/abcd3/\n",
        "osf = OSF()\n",
        "project = osf.project(project_id)\n",
        "store = project.storage(\"osfstorage\")\n",
        "\n",
        "desc_folder = []\n",
        "for fold in store.folders:\n",
        "    if fold.path.strip(\"/\") == \"Descriptors Data\":\n",
        "        desc_folder.append(fold)\n",
        "        break\n",
        "\n",
        "\n",
        "# Download all files and keep folder structure\n",
        "for folder in desc_folder:\n",
        "  for f in folder.files:\n",
        "      local_path = f.path.strip(\"/\")            # keep folders\n",
        "      local_dir = os.path.dirname(local_path)   # extract dir\n",
        "      if local_dir and not os.path.exists(local_dir):\n",
        "          os.makedirs(local_dir, exist_ok=True) # create dirs if missing\n",
        "      with open(local_path, \"wb\") as out:\n",
        "          f.write_to(out)\n",
        "      if local_path.endswith(\".zip\"):\n",
        "        command = f\"unzip '{local_path}' -d '{local_dir}'\"\n",
        "        run(command, shell=True)\n",
        "        print(f\"\\nUnzipped {local_path} -> {local_dir}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MGjmH9D2EEj4",
        "outputId": "8472903d-ce80-428b-8ab6-2d376be7e7f6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting osfclient\n",
            "  Downloading osfclient-0.0.5-py2.py3-none-any.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from osfclient) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from osfclient) (4.67.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from osfclient) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->osfclient) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->osfclient) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->osfclient) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->osfclient) (2025.8.3)\n",
            "Downloading osfclient-0.0.5-py2.py3-none-any.whl (39 kB)\n",
            "Installing collected packages: osfclient\n",
            "Successfully installed osfclient-0.0.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 23.8M/23.8M [00:00<00:00, 43.6Mbytes/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Unzipped Descriptors Data/Descriptors Data.zip -> Descriptors Data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Requirements: xgboost, scikit-learn, numpy, pandas, joblib\n",
        "import os\n",
        "import random\n",
        "import json\n",
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "# --------------------------------------------------------------------\n",
        "# Config\n",
        "# --------------------------------------------------------------------\n",
        "RANDOM_STATE = 42\n",
        "np.random.seed(RANDOM_STATE)\n",
        "\n",
        "# -------------------------\n",
        "# User configuration\n",
        "# -------------------------\n",
        "DATA_DIR = Path(\"Descriptors Data\")   # expects files like data/BP_MACCS.csv or data/BP_MACCS_training_log.csv? -> use descriptor CSVs\n",
        "OUT_DIR = Path(\"xgb_cv_results\")\n",
        "OUT_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# Filename convention: {property}_{descriptor}.csv\n",
        "# CSV must contain feature columns + a column \"target\"\n",
        "# If SMILES present, drop it before training\n",
        "def load_xy(property_name, descriptor_name, data_dir=DATA_DIR, target_col=\"target\"):\n",
        "    fn = data_dir / f\"{property_name}_{descriptor_name}.parquet\"\n",
        "    if not fn.exists():\n",
        "        raise FileNotFoundError(f\"Expected file: {fn}\")\n",
        "    df = pd.read_parquet(fn)\n",
        "    # if target_col not in df.columns:\n",
        "    #     raise ValueError(f\"target column '{target_col}' not found in {fn}. Found columns: {df.columns.tolist()}\")\n",
        "    X = df.iloc[:, 9:].values   # descriptors columns starts from 10th column onward\n",
        "    y = df.iloc[:, 5].values   # 6th column contains the target property\n",
        "    return X, y, df\n",
        "\n",
        "# Param space for random search (XGBoost constructor arguments)\n",
        "param_dist_xgb = {\n",
        "    \"n_estimators\": [100, 200, 400, 800],\n",
        "    \"max_depth\": [3, 5, 7, 9],\n",
        "    \"learning_rate\": [0.01, 0.03, 0.05, 0.1],\n",
        "    \"subsample\": [0.6, 0.8, 1.0],\n",
        "    \"colsample_bytree\": [0.6, 0.8, 1.0],\n",
        "    \"reg_alpha\": [0.0, 0.5, 1.0],\n",
        "    \"reg_lambda\": [1.0, 3.0, 5.0],\n",
        "    # you can add more if you like\n",
        "}\n",
        "\n",
        "def sample_params(param_dist):\n",
        "    return {k: random.choice(v) for k, v in param_dist.items()}\n",
        "\n",
        "# --------------------------------------------------------------------\n",
        "# Inner CV evaluator that uses early stopping\n",
        "# --------------------------------------------------------------------\n",
        "def evaluate_params_with_inner_cv(X, y, params,\n",
        "                                  inner_splits=3, random_state=RANDOM_STATE,\n",
        "                                  early_stopping_rounds=30):\n",
        "    \"\"\"\n",
        "    For one hyperparameter configuration, perform inner k-fold CV with early stopping.\n",
        "    Returns the mean validation MAE across inner folds.\n",
        "    \"\"\"\n",
        "    kf = KFold(n_splits=inner_splits, shuffle=True, random_state=random_state)\n",
        "    maes = []\n",
        "\n",
        "    for fold, (tr_idx, val_idx) in enumerate(kf.split(X, y), start=1):\n",
        "        X_tr, X_val = X[tr_idx], X[val_idx]\n",
        "        y_tr, y_val = y[tr_idx], y[val_idx]\n",
        "\n",
        "        # scale features using scaler fit on train fold\n",
        "        scaler = StandardScaler()\n",
        "        X_tr_s = scaler.fit_transform(X_tr)\n",
        "        X_val_s = scaler.transform(X_val)\n",
        "\n",
        "        # ensure eval_metric is provided in constructor (avoid passing to fit)\n",
        "        model = XGBRegressor(\n",
        "            objective=\"reg:squarederror\",\n",
        "            n_jobs=-1,\n",
        "            random_state=int(random_state + fold),\n",
        "            eval_metric=\"mae\",\n",
        "            early_stopping_rounds=early_stopping_rounds,\n",
        "            **params\n",
        "        )\n",
        "\n",
        "        # fit with early stopping monitoring the validation fold (MAE)\n",
        "        model.fit(\n",
        "            X_tr_s, y_tr,\n",
        "            eval_set=[(X_val_s, y_val)],\n",
        "            verbose=False\n",
        "        )\n",
        "\n",
        "        y_val_pred = model.predict(X_val_s)\n",
        "        maes.append(mean_absolute_error(y_val, y_val_pred))\n",
        "\n",
        "    return float(np.mean(maes)), float(np.std(maes))\n",
        "\n",
        "# --------------------------------------------------------------------\n",
        "# Outer loop: nested CV with random search + early stopping\n",
        "# --------------------------------------------------------------------\n",
        "def nested_cv_xgb_earlystop(X, y,\n",
        "                            outer_splits=5, inner_splits=3,\n",
        "                            n_iter_search=15, random_state=RANDOM_STATE,\n",
        "                            out_prefix=\"exp\"):\n",
        "    \"\"\"\n",
        "    Runs nested CV:\n",
        "      - outer KFold for evaluation\n",
        "      - inner KFold for hyperparam search (random sampling) with early stopping\n",
        "    Returns:\n",
        "      - df_folds: per-fold metrics and params\n",
        "      - summary: aggregated metrics\n",
        "    \"\"\"\n",
        "    outer_cv = KFold(n_splits=outer_splits, shuffle=True, random_state=random_state)\n",
        "    results = []\n",
        "\n",
        "    for fold_idx, (train_idx, test_idx) in enumerate(outer_cv.split(X, y), start=1):\n",
        "        print(f\"\\n--- Outer fold {fold_idx}/{outer_splits} ---\")\n",
        "        X_train, X_test = X[train_idx], X[test_idx]\n",
        "        y_train, y_test = y[train_idx], y[test_idx]\n",
        "\n",
        "        # Random search over param_dist_xgb\n",
        "        best_score = float(\"inf\")\n",
        "        best_params = None\n",
        "        best_std = None\n",
        "\n",
        "        for i in range(n_iter_search):\n",
        "            candidate = sample_params(param_dist_xgb)\n",
        "            mean_mae, std_mae = evaluate_params_with_inner_cv(\n",
        "                X_train, y_train, params=candidate,\n",
        "                inner_splits=inner_splits,\n",
        "                random_state=random_state + i,\n",
        "                early_stopping_rounds=30\n",
        "            )\n",
        "            # lower MAE is better\n",
        "            if mean_mae < best_score:\n",
        "                best_score = mean_mae\n",
        "                best_params = candidate\n",
        "                best_std = std_mae\n",
        "\n",
        "            if (i+1) % max(1, n_iter_search//5) == 0:\n",
        "                print(f\"  Tried {i+1}/{n_iter_search} candidates, best MAE so far: {best_score:.4f}\")\n",
        "\n",
        "        print(f\"> Best inner MAE (avg) for fold {fold_idx}: {best_score:.4f} ± {best_std:.4f}\")\n",
        "        print(\"  Best params:\", best_params)\n",
        "\n",
        "        # Retrain best on the entire outer training set, with a small held-out validation for early stopping\n",
        "        X_tr_final, X_val_final, y_tr_final, y_val_final = train_test_split(\n",
        "            X_train, y_train, test_size=0.15, random_state=random_state\n",
        "        )\n",
        "\n",
        "        scaler_final = StandardScaler()\n",
        "        X_tr_f_s = scaler_final.fit_transform(X_tr_final)\n",
        "        X_val_f_s = scaler_final.transform(X_val_final)\n",
        "        X_test_s    = scaler_final.transform(X_test)\n",
        "\n",
        "        best_model = XGBRegressor(\n",
        "            objective=\"reg:squarederror\",\n",
        "            n_jobs=-1,\n",
        "            random_state=int(random_state + fold_idx),\n",
        "            eval_metric=\"mae\",\n",
        "            early_stopping_rounds=30,\n",
        "            **best_params\n",
        "        )\n",
        "\n",
        "        best_model.fit(\n",
        "            X_tr_f_s, y_tr_final,\n",
        "            eval_set=[(X_val_f_s, y_val_final)],\n",
        "            verbose=False\n",
        "        )\n",
        "\n",
        "        y_test_pred = best_model.predict(X_test_s)\n",
        "        mae = mean_absolute_error(y_test, y_test_pred)\n",
        "        rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
        "        r2 = r2_score(y_test, y_test_pred)\n",
        "\n",
        "        # feature importances (aligned with columns in X)\n",
        "        try:\n",
        "            fi = best_model.feature_importances_.tolist()\n",
        "        except Exception:\n",
        "            fi = None\n",
        "\n",
        "        # Save scaler + model together\n",
        "        joblib.dump({\"scaler\": scaler_final, \"model\": best_model},\n",
        "                    OUT_DIR / f\"{out_prefix}_fold{fold_idx}_pipeline.joblib\")\n",
        "\n",
        "        results.append({\n",
        "            \"fold\": fold_idx,\n",
        "            \"mae\": float(mae),\n",
        "            \"rmse\": float(rmse),\n",
        "            \"r2\": float(r2),\n",
        "            \"best_inner_mae\": float(best_score),\n",
        "            \"best_inner_mae_std\": float(best_std),\n",
        "            \"best_params\": best_params,\n",
        "            \"feature_importances\": fi\n",
        "        })\n",
        "\n",
        "    df_res = pd.DataFrame(results)\n",
        "    summary = {\n",
        "        \"mae_mean\": df_res[\"mae\"].mean(),\n",
        "        \"mae_std\": df_res[\"mae\"].std(),\n",
        "        \"rmse_mean\": df_res[\"rmse\"].mean(),\n",
        "        \"r2_mean\": df_res[\"r2\"].mean()\n",
        "    }\n",
        "    return df_res, summary\n",
        "\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Sweep over all property/descriptor combos\n",
        "# -------------------------\n",
        "\n",
        "# properties = [\"Log VP\", \"MP\", \"BP\", \"LogBCF\", \"LogS\", \"LogP\"]\n",
        "properties = [\"LogP\"]\n",
        "descriptors = [\"MACCS\", \"Morgan\", \"pwav\"]\n",
        "# descriptors = [\"MACCS\"]\n",
        "\n",
        "all_experiments = []\n",
        "for prop in properties:\n",
        "    for desc in descriptors:\n",
        "        print(f\">>> Running nested CV for {prop} | {desc}\")\n",
        "        try:\n",
        "            X, y, raw_df = load_xy(prop, desc)\n",
        "        except Exception as e:\n",
        "            print(f\"Skipping {prop}_{desc} because: {e}\")\n",
        "            continue\n",
        "\n",
        "        out_prefix = f\"{prop}_{desc}\"\n",
        "        per_fold_df, per_exp_summary = nested_cv_xgb_earlystop(X, y,\n",
        "                            outer_splits=5, inner_splits=3,\n",
        "                            n_iter_search=50, random_state=RANDOM_STATE,\n",
        "                            out_prefix=out_prefix)\n",
        "\n",
        "        # save outputs\n",
        "        per_fold_df.to_csv(OUT_DIR / f\"{out_prefix}_cv_folds.csv\", index=False)\n",
        "        with open(OUT_DIR / f\"{out_prefix}_summary.json\", \"w\") as fh:\n",
        "            json.dump(per_exp_summary, fh, indent=2)\n",
        "\n",
        "        # append a flat record for later aggregation\n",
        "        all_experiments.append({\n",
        "            \"Property\": prop,\n",
        "            \"Descriptor\": desc,\n",
        "            \"mae_mean\": per_exp_summary[\"mae_mean\"],\n",
        "            \"mae_std\": per_exp_summary[\"mae_std\"],\n",
        "            \"rmse_mean\": per_exp_summary[\"rmse_mean\"],\n",
        "            \"r2_mean\": per_exp_summary[\"r2_mean\"]\n",
        "        })\n",
        "\n",
        "# Save global table\n",
        "pd.DataFrame(all_experiments).to_csv(OUT_DIR / \"all_xgb_results_summary.csv\", index=False)\n",
        "print(\"All experiments finished. Results saved to\", OUT_DIR)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVc5d6cyEIu8",
        "outputId": "2edd429c-8355-4fc0-ec07-3981d1178cc6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Running nested CV for LogP | MACCS\n",
            "\n",
            "--- Outer fold 1/5 ---\n",
            "  Tried 10/50 candidates, best MAE so far: 0.4899\n",
            "  Tried 20/50 candidates, best MAE so far: 0.4861\n",
            "  Tried 30/50 candidates, best MAE so far: 0.4861\n",
            "  Tried 40/50 candidates, best MAE so far: 0.4861\n",
            "  Tried 50/50 candidates, best MAE so far: 0.4861\n",
            "> Best inner MAE (avg) for fold 1: 0.4861 ± 0.0072\n",
            "  Best params: {'n_estimators': 800, 'max_depth': 9, 'learning_rate': 0.03, 'subsample': 0.6, 'colsample_bytree': 0.8, 'reg_alpha': 0.5, 'reg_lambda': 1.0}\n",
            "\n",
            "--- Outer fold 2/5 ---\n",
            "  Tried 10/50 candidates, best MAE so far: 0.4945\n",
            "  Tried 20/50 candidates, best MAE so far: 0.4833\n",
            "  Tried 30/50 candidates, best MAE so far: 0.4833\n",
            "  Tried 40/50 candidates, best MAE so far: 0.4833\n",
            "  Tried 50/50 candidates, best MAE so far: 0.4833\n",
            "> Best inner MAE (avg) for fold 2: 0.4833 ± 0.0008\n",
            "  Best params: {'n_estimators': 800, 'max_depth': 7, 'learning_rate': 0.1, 'subsample': 1.0, 'colsample_bytree': 1.0, 'reg_alpha': 0.5, 'reg_lambda': 3.0}\n",
            "\n",
            "--- Outer fold 3/5 ---\n",
            "  Tried 10/50 candidates, best MAE so far: 0.4921\n",
            "  Tried 20/50 candidates, best MAE so far: 0.4921\n",
            "  Tried 30/50 candidates, best MAE so far: 0.4921\n",
            "  Tried 40/50 candidates, best MAE so far: 0.4921\n",
            "  Tried 50/50 candidates, best MAE so far: 0.4921\n",
            "> Best inner MAE (avg) for fold 3: 0.4921 ± 0.0062\n",
            "  Best params: {'n_estimators': 800, 'max_depth': 7, 'learning_rate': 0.1, 'subsample': 0.6, 'colsample_bytree': 1.0, 'reg_alpha': 1.0, 'reg_lambda': 3.0}\n",
            "\n",
            "--- Outer fold 4/5 ---\n",
            "  Tried 10/50 candidates, best MAE so far: 0.4831\n",
            "  Tried 20/50 candidates, best MAE so far: 0.4831\n",
            "  Tried 30/50 candidates, best MAE so far: 0.4831\n",
            "  Tried 40/50 candidates, best MAE so far: 0.4806\n",
            "  Tried 50/50 candidates, best MAE so far: 0.4806\n",
            "> Best inner MAE (avg) for fold 4: 0.4806 ± 0.0037\n",
            "  Best params: {'n_estimators': 800, 'max_depth': 7, 'learning_rate': 0.05, 'subsample': 0.6, 'colsample_bytree': 0.8, 'reg_alpha': 0.0, 'reg_lambda': 5.0}\n",
            "\n",
            "--- Outer fold 5/5 ---\n",
            "  Tried 10/50 candidates, best MAE so far: 0.4873\n",
            "  Tried 20/50 candidates, best MAE so far: 0.4873\n",
            "  Tried 30/50 candidates, best MAE so far: 0.4873\n",
            "  Tried 40/50 candidates, best MAE so far: 0.4873\n",
            "  Tried 50/50 candidates, best MAE so far: 0.4873\n",
            "> Best inner MAE (avg) for fold 5: 0.4873 ± 0.0060\n",
            "  Best params: {'n_estimators': 800, 'max_depth': 7, 'learning_rate': 0.1, 'subsample': 0.8, 'colsample_bytree': 1.0, 'reg_alpha': 1.0, 'reg_lambda': 5.0}\n",
            ">>> Running nested CV for LogP | Morgan\n",
            "\n",
            "--- Outer fold 1/5 ---\n",
            "  Tried 10/50 candidates, best MAE so far: 0.5982\n",
            "  Tried 20/50 candidates, best MAE so far: 0.5143\n",
            "  Tried 30/50 candidates, best MAE so far: 0.5143\n",
            "  Tried 40/50 candidates, best MAE so far: 0.5143\n",
            "  Tried 50/50 candidates, best MAE so far: 0.5143\n",
            "> Best inner MAE (avg) for fold 1: 0.5143 ± 0.0045\n",
            "  Best params: {'n_estimators': 800, 'max_depth': 9, 'learning_rate': 0.1, 'subsample': 0.8, 'colsample_bytree': 0.6, 'reg_alpha': 1.0, 'reg_lambda': 1.0}\n",
            "\n",
            "--- Outer fold 2/5 ---\n",
            "  Tried 10/50 candidates, best MAE so far: 0.5483\n",
            "  Tried 20/50 candidates, best MAE so far: 0.5483\n",
            "  Tried 30/50 candidates, best MAE so far: 0.5335\n",
            "  Tried 40/50 candidates, best MAE so far: 0.5087\n",
            "  Tried 50/50 candidates, best MAE so far: 0.5087\n",
            "> Best inner MAE (avg) for fold 2: 0.5087 ± 0.0057\n",
            "  Best params: {'n_estimators': 800, 'max_depth': 9, 'learning_rate': 0.1, 'subsample': 1.0, 'colsample_bytree': 0.6, 'reg_alpha': 1.0, 'reg_lambda': 3.0}\n",
            "\n",
            "--- Outer fold 3/5 ---\n",
            "  Tried 10/50 candidates, best MAE so far: 0.5268\n",
            "  Tried 20/50 candidates, best MAE so far: 0.5268\n",
            "  Tried 30/50 candidates, best MAE so far: 0.5268\n",
            "  Tried 40/50 candidates, best MAE so far: 0.5268\n",
            "  Tried 50/50 candidates, best MAE so far: 0.5268\n",
            "> Best inner MAE (avg) for fold 3: 0.5268 ± 0.0067\n",
            "  Best params: {'n_estimators': 800, 'max_depth': 9, 'learning_rate': 0.05, 'subsample': 0.6, 'colsample_bytree': 1.0, 'reg_alpha': 0.5, 'reg_lambda': 5.0}\n",
            "\n",
            "--- Outer fold 4/5 ---\n",
            "  Tried 10/50 candidates, best MAE so far: 0.5201\n",
            "  Tried 20/50 candidates, best MAE so far: 0.5201\n",
            "  Tried 30/50 candidates, best MAE so far: 0.5201\n",
            "  Tried 40/50 candidates, best MAE so far: 0.5201\n",
            "  Tried 50/50 candidates, best MAE so far: 0.5201\n",
            "> Best inner MAE (avg) for fold 4: 0.5201 ± 0.0055\n",
            "  Best params: {'n_estimators': 800, 'max_depth': 9, 'learning_rate': 0.1, 'subsample': 0.6, 'colsample_bytree': 1.0, 'reg_alpha': 0.5, 'reg_lambda': 5.0}\n",
            "\n",
            "--- Outer fold 5/5 ---\n",
            "  Tried 10/50 candidates, best MAE so far: 0.5280\n",
            "  Tried 20/50 candidates, best MAE so far: 0.5179\n",
            "  Tried 30/50 candidates, best MAE so far: 0.5179\n",
            "  Tried 40/50 candidates, best MAE so far: 0.5179\n",
            "  Tried 50/50 candidates, best MAE so far: 0.5179\n",
            "> Best inner MAE (avg) for fold 5: 0.5179 ± 0.0031\n",
            "  Best params: {'n_estimators': 800, 'max_depth': 7, 'learning_rate': 0.1, 'subsample': 1.0, 'colsample_bytree': 0.6, 'reg_alpha': 1.0, 'reg_lambda': 1.0}\n",
            ">>> Running nested CV for LogP | pwav\n",
            "\n",
            "--- Outer fold 1/5 ---\n",
            "  Tried 10/50 candidates, best MAE so far: 0.3998\n",
            "  Tried 20/50 candidates, best MAE so far: 0.3998\n",
            "  Tried 30/50 candidates, best MAE so far: 0.3998\n",
            "  Tried 40/50 candidates, best MAE so far: 0.3998\n",
            "  Tried 50/50 candidates, best MAE so far: 0.3998\n",
            "> Best inner MAE (avg) for fold 1: 0.3998 ± 0.0023\n",
            "  Best params: {'n_estimators': 800, 'max_depth': 7, 'learning_rate': 0.03, 'subsample': 0.6, 'colsample_bytree': 0.6, 'reg_alpha': 0.0, 'reg_lambda': 5.0}\n",
            "\n",
            "--- Outer fold 2/5 ---\n",
            "  Tried 10/50 candidates, best MAE so far: 0.4086\n",
            "  Tried 20/50 candidates, best MAE so far: 0.4036\n",
            "  Tried 30/50 candidates, best MAE so far: 0.4036\n",
            "  Tried 40/50 candidates, best MAE so far: 0.4036\n",
            "  Tried 50/50 candidates, best MAE so far: 0.4030\n",
            "> Best inner MAE (avg) for fold 2: 0.4030 ± 0.0074\n",
            "  Best params: {'n_estimators': 800, 'max_depth': 7, 'learning_rate': 0.05, 'subsample': 0.8, 'colsample_bytree': 0.6, 'reg_alpha': 0.5, 'reg_lambda': 1.0}\n",
            "\n",
            "--- Outer fold 3/5 ---\n",
            "  Tried 10/50 candidates, best MAE so far: 0.4104\n",
            "  Tried 20/50 candidates, best MAE so far: 0.4089\n",
            "  Tried 30/50 candidates, best MAE so far: 0.4076\n",
            "  Tried 40/50 candidates, best MAE so far: 0.4076\n",
            "  Tried 50/50 candidates, best MAE so far: 0.4073\n",
            "> Best inner MAE (avg) for fold 3: 0.4073 ± 0.0044\n",
            "  Best params: {'n_estimators': 800, 'max_depth': 7, 'learning_rate': 0.05, 'subsample': 0.6, 'colsample_bytree': 1.0, 'reg_alpha': 0.5, 'reg_lambda': 1.0}\n",
            "\n",
            "--- Outer fold 4/5 ---\n",
            "  Tried 10/50 candidates, best MAE so far: 0.4100\n",
            "  Tried 20/50 candidates, best MAE so far: 0.4100\n",
            "  Tried 30/50 candidates, best MAE so far: 0.3985\n",
            "  Tried 40/50 candidates, best MAE so far: 0.3985\n",
            "  Tried 50/50 candidates, best MAE so far: 0.3985\n",
            "> Best inner MAE (avg) for fold 4: 0.3985 ± 0.0034\n",
            "  Best params: {'n_estimators': 800, 'max_depth': 7, 'learning_rate': 0.05, 'subsample': 0.6, 'colsample_bytree': 0.6, 'reg_alpha': 0.5, 'reg_lambda': 3.0}\n",
            "\n",
            "--- Outer fold 5/5 ---\n",
            "  Tried 10/50 candidates, best MAE so far: 0.4028\n",
            "  Tried 20/50 candidates, best MAE so far: 0.4009\n",
            "  Tried 30/50 candidates, best MAE so far: 0.4009\n",
            "  Tried 40/50 candidates, best MAE so far: 0.4009\n",
            "  Tried 50/50 candidates, best MAE so far: 0.4009\n",
            "> Best inner MAE (avg) for fold 5: 0.4009 ± 0.0064\n",
            "  Best params: {'n_estimators': 800, 'max_depth': 7, 'learning_rate': 0.05, 'subsample': 0.8, 'colsample_bytree': 0.6, 'reg_alpha': 0.5, 'reg_lambda': 5.0}\n",
            "All experiments finished. Results saved to xgb_cv_results\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Still in Progress!"
      ],
      "metadata": {
        "id": "hOsrR1CdoSro"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zHCjSfwr-SUD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
