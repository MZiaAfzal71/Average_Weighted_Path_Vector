{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPh2JouDgHU1Md4t6Cy9Yl+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MZiaAfzal71/Average_Weighted_Path_Vector/blob/main/Data%20Files/Models/ChemBERTaModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R17zIeBUMfAx"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/MZiaAfzal71/Average_Weighted_Path_Vector.git\n",
        "%cd Average_Weighted_Path_Vector/Data\\ Files"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import os\n",
        "import random\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Optional, Union, Dict, Any\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, root_mean_squared_error\n",
        "from tqdm.auto import tqdm"
      ],
      "metadata": {
        "id": "4Fhykws8NHxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# Config\n",
        "# ----------------------------\n",
        "@dataclass\n",
        "class Config:\n",
        "    model_name: str = \"seyonec/ChemBERTa-zinc-base-v1\"\n",
        "    output_dir: str = \"./chemberta_model_output\"\n",
        "    save_path: str = \"best_model.pt\"\n",
        "    max_length: int = 128\n",
        "    batch_size: int = 16\n",
        "    epochs: int = 5\n",
        "    lr: float = 1e-5\n",
        "    weight_decay: float = 0.01\n",
        "    seed: int = 42\n",
        "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    dropout: float = 0.1\n",
        "    train_layers: int = 2   # unfreeze last N transformer blocks; 0 = all frozen\n",
        "    warmup_ratio: float = 0.1\n",
        "    grad_clip: float = 0.0\n",
        "    return_numpy: bool = True\n",
        "\n",
        "# ----------------------------\n",
        "# Utils\n",
        "# ----------------------------\n",
        "def set_seed(seed: int):\n",
        "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def rmse(y_true, y_pred):\n",
        "    return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
        "\n",
        "def ensure_dir(path: str):\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "\n",
        "# ----------------------------\n",
        "# Model\n",
        "# ----------------------------\n",
        "\n",
        "class ChemBERTaModel(nn.Module):\n",
        "    \"\"\"\n",
        "    ChemBERTa Model:\n",
        "    \"\"\"\n",
        "    def __init__(self, model_name: str, dropout: float = 0.1, train_layers: int = 0):\n",
        "        super().__init__()\n",
        "        self.bert = AutoModel.from_pretrained(model_name)\n",
        "        H = self.bert.config.hidden_size\n",
        "\n",
        "\n",
        "        # Freeze backbone, optionally unfreeze tail\n",
        "        # for p in self.bert.parameters():\n",
        "        #     p.requires_grad = False\n",
        "        # if train_layers and hasattr(self.bert, \"encoder\"):\n",
        "        #     for layer in self.bert.encoder.layer[-train_layers:]:\n",
        "        #         for p in layer.parameters():\n",
        "        #             p.requires_grad = True\n",
        "\n",
        "\n",
        "        # Main head on fused features\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Linear(H, H//2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(H//2, 1),\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, targets=None):\n",
        "        \"\"\"\n",
        "        Returns:\n",
        "          pred: [B]\n",
        "          loss (if targets given)\n",
        "        \"\"\"\n",
        "        out = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        cls = out.last_hidden_state[:, 0, :]  # [B,H]\n",
        "\n",
        "        y_pred = self.main(cls).squeeze(-1)\n",
        "\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            targets = targets.float()\n",
        "            loss = F.mse_loss(y_pred, targets)\n",
        "\n",
        "        return y_pred, loss\n",
        "\n",
        "# ----------------------------\n",
        "# Dataset / Collate\n",
        "# ----------------------------\n",
        "class SmileDataset(Dataset):\n",
        "    def __init__(self, smiles: List[str], targets: Optional[np.ndarray],\n",
        "                 tokenizer: AutoTokenizer, max_length: int):\n",
        "        self.smiles = list(smiles)\n",
        "        self.targets = None if targets is None else np.asarray(targets, dtype=np.float32)\n",
        "        self.tok = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self): return len(self.smiles)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        enc = self.tok(self.smiles[i],\n",
        "                       truncation=True, padding=\"max_length\",\n",
        "                       max_length=self.max_length, return_tensors=\"pt\")\n",
        "        item = {k: v.squeeze(0) for k, v in enc.items()}\n",
        "        if self.targets is not None:\n",
        "            item[\"labels\"] = torch.tensor(self.targets[i], dtype=torch.float32)\n",
        "        return item\n",
        "\n",
        "def collate_stack(batch):\n",
        "    out = {k: torch.stack([b[k] for b in batch]) for k in batch[0] if k != \"labels\"}\n",
        "    if \"labels\" in batch[0]:\n",
        "        out[\"labels\"] = torch.stack([b[\"labels\"] for b in batch])\n",
        "    return out\n",
        "\n",
        "\n",
        "def make_loaders(df: pd.DataFrame, target_col: str, tokenizer: AutoTokenizer,\n",
        "                 cfg: Config) -> Tuple[DataLoader, DataLoader]:\n",
        "    # Split\n",
        "    train_df = df[df[\"Training/Test\"].str.strip().str.lower() == \"training\"].reset_index(drop=True)\n",
        "    test_df  = df[df[\"Training/Test\"].str.strip().str.lower() == \"test\"].reset_index(drop=True)\n",
        "\n",
        "    train_ds = SmileDataset(train_df[\"SMILES\"].tolist(),\n",
        "                              train_df[target_col].to_numpy(dtype=np.float32),\n",
        "                              tokenizer, cfg.max_length)\n",
        "    test_ds  = SmileDataset(test_df[\"SMILES\"].tolist(),\n",
        "                              test_df[target_col].to_numpy(dtype=np.float32),\n",
        "                              tokenizer, cfg.max_length)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True,\n",
        "                              collate_fn=collate_stack)\n",
        "    test_loader  = DataLoader(test_ds, batch_size=cfg.batch_size, shuffle=False,\n",
        "                              collate_fn=collate_stack)\n",
        "    return train_loader, test_loader\n",
        "\n",
        "\n",
        "def setup_optimizer_scheduler(model, train_dataloader, epochs, lr=2e-5, warmup_ratio=0.1):\n",
        "\n",
        "    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()),\n",
        "                                  lr = lr, weight_decay=0.01)\n",
        "\n",
        "    total_steps = len(train_dataloader) * epochs\n",
        "    warmup_steps = int(total_steps * warmup_ratio)\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n",
        "\n",
        "    return optimizer, scheduler\n",
        "\n",
        "\n",
        "def train_model(model, train_loader, val_loader, optimizer, scheduler,\n",
        "                device, epochs=10, grad_clip=1.0, save_path=\"best_model.pt\"):\n",
        "    \"\"\"\n",
        "    Full trainer loop for ChemBERTaModel.\n",
        "\n",
        "    Args:\n",
        "      model: nn.Module\n",
        "      train_loader: DataLoader\n",
        "      val_loader: DataLoader\n",
        "      optimizer: torch.optim.Optimizer\n",
        "      scheduler: torch.optim.lr_scheduler\n",
        "      device: torch.device\n",
        "      epochs: int\n",
        "      grad_clip: float (gradient clipping norm)\n",
        "      save_path: str (where to save best model)\n",
        "    \"\"\"\n",
        "    model.to(device)\n",
        "    best_val = float(\"inf\")\n",
        "\n",
        "    for epoch in range(1, epochs+1):\n",
        "        # ---- TRAIN ----\n",
        "        model.train()\n",
        "        train_loss, n_train = 0.0, 0\n",
        "        diag_accum = {}\n",
        "\n",
        "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch} [Train]\")\n",
        "        for batch in pbar:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            targets = batch[\"labels\"].to(device)\n",
        "\n",
        "            preds, loss= model(input_ids, attention_mask, targets)\n",
        "\n",
        "            loss.backward()\n",
        "            if grad_clip:\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            bs = input_ids.size(0)\n",
        "            train_loss += loss.item() * bs\n",
        "            n_train += bs\n",
        "\n",
        "            pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
        "\n",
        "        train_loss /= n_train\n",
        "        diag_accum = {k: v / n_train for k, v in diag_accum.items()}\n",
        "\n",
        "        # ---- VALIDATION ----\n",
        "        model.eval()\n",
        "        val_loss, n_val = 0.0, 0\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(val_loader, desc=f\"Epoch {epoch} [Val]\"):\n",
        "                input_ids = batch[\"input_ids\"].to(device)\n",
        "                attention_mask = batch[\"attention_mask\"].to(device)\n",
        "                targets = batch[\"labels\"].to(device)\n",
        "\n",
        "                preds, loss = model(input_ids, attention_mask, targets)\n",
        "\n",
        "                bs = input_ids.size(0)\n",
        "                val_loss += loss.item() * bs\n",
        "                n_val += bs\n",
        "\n",
        "        val_loss /= n_val\n",
        "\n",
        "        print(f\"Epoch {epoch}: train_loss={train_loss:.4f}, val_loss={val_loss:.4f}, \")\n",
        "\n",
        "        # ---- Save best ----\n",
        "        if val_loss < best_val:\n",
        "            best_val = val_loss\n",
        "            torch.save(model.state_dict(), save_path)\n",
        "            print(f\"✅ Saved best model (val_loss={val_loss:.4f})\")\n",
        "\n",
        "    print(\"Training complete.\")\n",
        "    model.load_state_dict(torch.load(save_path))\n",
        "    return model\n",
        "\n",
        "def predict(model, test_loader, device, return_numpy=True):\n",
        "    \"\"\"\n",
        "    Run inference on a test set.\n",
        "\n",
        "    Args:\n",
        "      model: trained ChemBERTaFusionV2\n",
        "      test_loader: DataLoader\n",
        "      device: torch.device\n",
        "      return_numpy: if True, returns numpy array\n",
        "\n",
        "    Returns:\n",
        "      preds: [N] predictions (torch.Tensor or np.ndarray)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "    all_preds = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "\n",
        "            preds, _ = model(input_ids, attention_mask)\n",
        "            all_preds.append(preds.cpu())\n",
        "\n",
        "    preds = torch.cat(all_preds, dim=0)\n",
        "\n",
        "    if return_numpy:\n",
        "        return preds.numpy()\n",
        "    return preds\n",
        "\n",
        "def train_for_prop(file: str, prop: str, cfg: Config) -> Dict[str, Any]:\n",
        "    set_seed(cfg.seed)\n",
        "    ensure_dir(cfg.output_dir)\n",
        "\n",
        "    # ---- Load data\n",
        "    target_col = f\"{prop}-Measured\"\n",
        "\n",
        "    try:\n",
        "        df = pd.read_excel(file, sheet_name=prop)\n",
        "    except:\n",
        "        raise ValueError(f\"{data_file} is not found.\")\n",
        "\n",
        "\n",
        "\n",
        "    # ---- Tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(cfg.model_name, use_fast=True)\n",
        "\n",
        "    # ---- Data loaders\n",
        "    train_loader, test_loader = make_loaders(df, target_col, tokenizer, cfg)\n",
        "\n",
        "    # ---- ChemBerta Model\n",
        "    model = ChemBERTaModel(cfg.model_name, dropout=cfg.dropout,\n",
        "                           train_layers=cfg.train_layers).to(cfg.device)\n",
        "\n",
        "    # Parameter groups (smaller LR for backbone; larger for fusion head/regressor)\n",
        "    optimizer, scheduler = setup_optimizer_scheduler(model, train_loader, cfg.epochs,\n",
        "                                                    cfg.lr, cfg.warmup_ratio)\n",
        "\n",
        "    save_model = os.path.join(cfg.output_dir, cfg.save_path)\n",
        "    model = train_model(model, train_loader, test_loader, optimizer, scheduler, cfg.device,\n",
        "               cfg.epochs, cfg.grad_clip, save_model)\n",
        "\n",
        "    # Predict on all rows (Training + Test)\n",
        "\n",
        "    tokenizer_fast = tokenizer  # reuse\n",
        "    all_ds = SmileDataset(df[\"SMILES\"].tolist(),\n",
        "                            df[target_col].to_numpy(dtype=np.float32),\n",
        "                            tokenizer_fast, cfg.max_length)\n",
        "    all_loader = DataLoader(all_ds, batch_size=cfg.batch_size, shuffle=False,\n",
        "                            collate_fn=collate_stack)\n",
        "\n",
        "    all_preds = predict(model, all_loader, cfg.device, cfg.return_numpy)\n",
        "\n",
        "    # Build results DF\n",
        "    new_results = df.copy()\n",
        "\n",
        "    new_results[f\"{prop} Prediction\"] = all_preds\n",
        "\n",
        "    # Final metrics on Test only\n",
        "    obs_test = new_results[new_results[\"Training/Test\"].str.lower() == \"test\"][target_col].values\n",
        "    pred_test = new_results[new_results[\"Training/Test\"].str.lower() == \"test\"][f\"{prop} Prediction\"].values\n",
        "    mae_v = mean_absolute_error(obs_test, pred_test)\n",
        "    rmse_v = rmse(obs_test, pred_test)\n",
        "    r2_v = r2_score(obs_test, pred_test)\n",
        "    print(f\"Final (best) Test metrics for {prop} → MAE: {mae_v:.4f} | RMSE: {rmse_v:.4f} | R²: {r2_v:.4f}\")\n",
        "\n",
        "    # Save predictions parquet\n",
        "    pred_path = os.path.join(cfg.output_dir, f\"{prop}_chemBERTa_preds.xlsx\")\n",
        "    new_results.to_excel(pred_path, index=False)\n",
        "    print(f\"Saved predictions → {pred_path}\")\n",
        "\n",
        "    return {\n",
        "        \"sheet\": {prop},\n",
        "        \"target_col\": target_col,\n",
        "        \"best_path\": save_model,\n",
        "        \"pred_path\": pred_path,\n",
        "        \"MAE\": mae_v, \"RMSE\": rmse_v, \"R2\": r2_v,\n",
        "    }\n",
        "\n",
        "# ----------------------------\n",
        "# Multi-property runner\n",
        "# ----------------------------\n",
        "def run_all_properties_descriptors(file: str, prop_names: str, cfg: Config):\n",
        "    ensure_dir(cfg.output_dir)\n",
        "    perf_rows = []\n",
        "    for prop in prop_names:\n",
        "        print(f\"\\n=== Processing the file Zang_Data for sheet: {prop} ===\")\n",
        "        results = train_for_prop(file, prop, cfg)\n",
        "        perf_rows.append([f\"{prop}\", results[\"MAE\"], results[\"RMSE\"], results[\"R2\"]])\n",
        "    perf_df = pd.DataFrame(perf_rows, columns=[\"Property\", \"MAE\", \"RMSE\", \"R2\"])\n",
        "    stats_path = os.path.join(cfg.output_dir, \"chemberta_stats.csv\")\n",
        "    perf_df.to_csv(stats_path, index=False)\n",
        "    print(f\"\\n📊 All-property stats saved → {stats_path}\")\n",
        "    return perf_df\n"
      ],
      "metadata": {
        "id": "Tqrxuns0NVab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cfg = Config(\n",
        "    output_dir=\"chemberta_results\",\n",
        "    epochs=30,\n",
        "    batch_size=8,\n",
        "    max_length=128,\n",
        "    train_layers=3,      # unfreeze last 3 transformer blocks\n",
        "    lr=1e-5,\n",
        ")\n",
        "\n",
        "file = \"Excel Files/Zang_Data.xlsx\"\n",
        "prop_names = [\"Log VP\", \"MP\", \"BP\", \"LogBCF\", \"LogS\", \"LogP\"]\n",
        "# prop_names = [\"LogP\"]\n",
        "perf_df = run_all_properties_descriptors(file, prop_names, cfg)\n",
        "\n",
        "perf_df"
      ],
      "metadata": {
        "id": "kIzisT_S3eF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cUpHQpnL4Hv1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
