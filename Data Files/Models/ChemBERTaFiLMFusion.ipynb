{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNyx/+Sfry8/v1tvQp66zv/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MZiaAfzal71/Average_Weighted_Path_Vector/blob/main/Data%20Files/Models/ChemBERTaFiLMFusion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OnAim3WqyTfy"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/MZiaAfzal71/Average_Weighted_Path_Vector.git\n",
        "%cd Average_Weighted_Path_Vector/Data\\ Files"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install osfclient\n",
        "from osfclient.api import OSF\n",
        "import os\n",
        "from subprocess import run\n",
        "\n",
        "# Replace with your OSF project ID\n",
        "project_id = \"p5ga2\"   # e.g. from https://osf.io/abcd3/\n",
        "osf = OSF()\n",
        "project = osf.project(project_id)\n",
        "store = project.storage(\"osfstorage\")\n",
        "\n",
        "desc_folder = []\n",
        "for fold in store.folders:\n",
        "    if fold.path.strip(\"/\") == \"Descriptors Data\":\n",
        "        desc_folder = fold\n",
        "        break\n",
        "\n",
        "# Download all files and keep folder structure\n",
        "for f in desc_folder.files:\n",
        "    local_path = f.path.strip(\"/\")            # keep folders\n",
        "    local_dir = os.path.dirname(local_path)   # extract dir\n",
        "    if local_dir and not os.path.exists(local_dir):\n",
        "        os.makedirs(local_dir, exist_ok=True) # create dirs if missing\n",
        "    with open(local_path, \"wb\") as out:\n",
        "        f.write_to(out)\n",
        "    if local_path.endswith(\".zip\"):\n",
        "      command = f\"unzip '{local_path}' -d '{local_dir}'\"\n",
        "      run(command, shell=True)\n",
        "      print(f\"\\nUnzipped {local_path} -> {local_dir}\")\n",
        "      continue\n",
        "    print(f\"Downloaded {f.path} -> {local_path}\")"
      ],
      "metadata": {
        "id": "TMaeCf-20Oqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "# ChemBERTa + Descriptor Gated Training Pipeline\n",
        "# ============================\n",
        "from __future__ import annotations\n",
        "import os, random, math, json, gc\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Optional, Dict, Any, Tuple\n",
        "\n",
        "import csv\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "from tqdm.auto import tqdm"
      ],
      "metadata": {
        "id": "iAK6_4wjzJiC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# Config\n",
        "# ----------------------------\n",
        "@dataclass\n",
        "class Config:\n",
        "    model_name: str = \"seyonec/ChemBERTa-zinc-base-v1\"\n",
        "    log_path: str = \"training_log.csv\"\n",
        "    output_dir: str = \"./chemberta_gated_out\"\n",
        "    save_path: str = \"best_model.pt\"\n",
        "    max_length: int = 128\n",
        "    batch_size: int = 16\n",
        "    epochs: int = 5\n",
        "    lr_backbone: float = 1e-5\n",
        "    lr_heads: float = 1e-4\n",
        "    weight_decay: float = 0.01\n",
        "    seed: int = 42\n",
        "    hidden_fuse: int = 512\n",
        "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    proj_dim: int = 128     # descriptor projection dimension before fusion\n",
        "    dropout: float = 0.1\n",
        "    train_layers: int = 2   # unfreeze last N transformer blocks; 0 = all frozen\n",
        "    gate_temp: float = 1.0\n",
        "    p_moddrop: float = 0.2\n",
        "    warmup_ratio: float = 0.1\n",
        "    grad_clip: float = 1.0\n",
        "    lambda_aux: float = 0.2\n",
        "    lambda_div: float = 0.05\n",
        "    return_numpy: bool = True\n",
        "\n",
        "# ----------------------------\n",
        "# Utils\n",
        "# ----------------------------\n",
        "def set_seed(seed: int):\n",
        "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def rmse(y_true, y_pred):\n",
        "    return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
        "\n",
        "def ensure_dir(path: str):\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "\n",
        "# ----------------------------\n",
        "# Model (gated fusion)\n",
        "# ----------------------------\n",
        "\n",
        "class ChemBERTaFusion(nn.Module):\n",
        "    \"\"\"\n",
        "    ChemBERTa + descriptors with:\n",
        "      - Modality dropout (forces both branches to carry signal)\n",
        "      - Data-dependent vector gate g = sigmoid(MLP([cls, desc_h]))\n",
        "      - FiLM conditioning of CLS with descriptors\n",
        "      - Auxiliary heads for CLS-only and DESC-only predictions\n",
        "      - Diversity regularizer (cosine similarity penalty)\n",
        "    \"\"\"\n",
        "    def __init__(self, model_name: str, n_desc: int,\n",
        "                 proj_dim: int = 128, hidden_fuse: int = 512,\n",
        "                 dropout: float = 0.1, train_layers: int = 0,\n",
        "                 gate_temp: float = 1.0,\n",
        "                 p_moddrop: float = 0.2):\n",
        "        super().__init__()\n",
        "        self.backbone = AutoModel.from_pretrained(model_name)\n",
        "        H = self.backbone.config.hidden_size\n",
        "        self.n_desc = n_desc\n",
        "        self.gate_temp = gate_temp\n",
        "        self.p_moddrop = p_moddrop\n",
        "\n",
        "        # Freeze backbone, optionally unfreeze tail\n",
        "        # for p in self.backbone.parameters():\n",
        "        #     p.requires_grad = False\n",
        "        # if train_layers and hasattr(self.backbone, \"encoder\"):\n",
        "        #     for layer in self.backbone.encoder.layer[-train_layers:]:\n",
        "        #         for p in layer.parameters():\n",
        "        #             p.requires_grad = True\n",
        "\n",
        "        # Descriptor projection -> hidden size\n",
        "        self.desc_proj = nn.Sequential(\n",
        "            nn.Linear(n_desc, proj_dim),\n",
        "            nn.LayerNorm(proj_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(proj_dim, H),\n",
        "            nn.LayerNorm(H),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        # Data-dependent gate: [cls, desc] -> H\n",
        "        self.gate_mlp = nn.Sequential(\n",
        "            nn.Linear(2*H, hidden_fuse),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_fuse, H)\n",
        "        )\n",
        "\n",
        "        # FiLM conditioning: desc -> (gamma, beta) in R^H\n",
        "        self.film = nn.Sequential(\n",
        "            nn.Linear(H, 2*H)\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Main head on fused features\n",
        "        self.head_main = nn.Sequential(\n",
        "            nn.Linear(H, H//2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(H//2, 1),\n",
        "        )\n",
        "\n",
        "        # Aux heads to force each branch to be predictive\n",
        "        self.head_cls  = nn.Linear(H, 1)\n",
        "        self.head_desc = nn.Linear(H, 1)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, descriptors,\n",
        "                targets=None, lambda_aux=0.2, lambda_div=0.05):\n",
        "        \"\"\"\n",
        "        Returns:\n",
        "          pred: [B]\n",
        "          loss (if targets given)\n",
        "          diagnostics dict\n",
        "        \"\"\"\n",
        "        B = input_ids.size(0)\n",
        "        out = self.backbone(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        cls = out.last_hidden_state[:, 0, :]  # [B,H]\n",
        "\n",
        "        desc_h = self.desc_proj(descriptors)  # [B,H]\n",
        "\n",
        "        # --- Modality dropout (train-time) ---\n",
        "        if self.training and self.p_moddrop > 0.0:\n",
        "            mask_choice = torch.empty(B, 1, device=cls.device).uniform_(0, 1)\n",
        "            drop_cls  = (mask_choice < self.p_moddrop).float()          # with prob p, drop CLS\n",
        "            drop_desc = ((mask_choice > 1-self.p_moddrop)).float()      # with prob p, drop DESC\n",
        "            cls  = cls  * (1.0 - drop_cls)\n",
        "            desc_h = desc_h * (1.0 - drop_desc)\n",
        "\n",
        "        # --- FiLM conditioning ---\n",
        "        gamma_beta = self.film(desc_h)           # [B,2H]\n",
        "        gamma, beta = torch.chunk(gamma_beta, 2, dim=-1)\n",
        "        cls_mod = (1.0 + gamma) * cls + beta     # reshaped CLS\n",
        "\n",
        "        # --- Data-dependent vector gate ---\n",
        "        gate_logits = self.gate_mlp(torch.cat([cls_mod, desc_h], dim=-1)) / self.gate_temp\n",
        "        g = torch.sigmoid(gate_logits)           # [B,H]\n",
        "\n",
        "        fused = g * cls_mod + (1.0 - g) * desc_h\n",
        "        fused = self.dropout(fused)\n",
        "\n",
        "        y_main = self.head_main(fused).squeeze(-1)\n",
        "        y_cls  = self.head_cls(cls_mod).squeeze(-1)\n",
        "        y_desc = self.head_desc(desc_h).squeeze(-1)\n",
        "\n",
        "        loss = None\n",
        "        diag = {}\n",
        "        if targets is not None:\n",
        "            targets = targets.float()\n",
        "            l_main = F.mse_loss(y_main, targets)\n",
        "            l_cls  = F.mse_loss(y_cls, targets)\n",
        "            l_desc = F.mse_loss(y_desc, targets)\n",
        "\n",
        "            # Diversity penalty: discourage aligned features\n",
        "            # (cosine sim averaged over batch)\n",
        "            eps = 1e-8\n",
        "            c_sim = F.cosine_similarity(\n",
        "                F.normalize(cls_mod, dim=-1, eps=eps),\n",
        "                F.normalize(desc_h, dim=-1, eps=eps), dim=-1\n",
        "            ).mean()\n",
        "\n",
        "            loss = l_main + lambda_aux * (l_cls + l_desc) + lambda_div * c_sim\n",
        "\n",
        "            diag = {\n",
        "                \"loss_main\": l_main.detach().item(),\n",
        "                \"loss_cls\":  l_cls.detach().item(),\n",
        "                \"loss_desc\": l_desc.detach().item(),\n",
        "                \"cos_sim\":   c_sim.detach().item(),\n",
        "                \"gate_mean\": g.mean().detach().item()\n",
        "            }\n",
        "\n",
        "        return y_main, loss, diag\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Dataset / Collate\n",
        "# ----------------------------\n",
        "class SmiDescDataset(Dataset):\n",
        "    def __init__(self, smiles: List[str], targets: Optional[np.ndarray],\n",
        "                 tokenizer: AutoTokenizer, max_length: int,\n",
        "                 descriptors: Optional[np.ndarray] = None):\n",
        "        self.smiles = list(smiles)\n",
        "        self.targets = None if targets is None else np.asarray(targets, dtype=np.float32)\n",
        "        self.tok = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.desc = None if descriptors is None else np.asarray(descriptors, dtype=np.float32)\n",
        "\n",
        "    def __len__(self): return len(self.smiles)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        enc = self.tok(self.smiles[i],\n",
        "                       truncation=True, padding=\"max_length\",\n",
        "                       max_length=self.max_length, return_tensors=\"pt\")\n",
        "        item = {k: v.squeeze(0) for k, v in enc.items()}\n",
        "        if self.targets is not None:\n",
        "            item[\"labels\"] = torch.tensor(self.targets[i], dtype=torch.float32)\n",
        "        if self.desc is not None:\n",
        "            # ensure finite floats\n",
        "            d = self.desc[i]\n",
        "            if not np.all(np.isfinite(d)):  # replace bad values\n",
        "                d = np.nan_to_num(d, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "            item[\"descriptors\"] = torch.tensor(d, dtype=torch.float32)\n",
        "        return item\n",
        "\n",
        "def collate_stack(batch):\n",
        "    out = {k: torch.stack([b[k] for b in batch]) for k in batch[0] if k != \"labels\"}\n",
        "    if \"labels\" in batch[0]:\n",
        "        out[\"labels\"] = torch.stack([b[\"labels\"] for b in batch])\n",
        "    return out\n",
        "\n",
        "\n",
        "def make_loaders(df: pd.DataFrame, target_col: str, tokenizer: AutoTokenizer,\n",
        "                 cfg: Config, scaler: Optional[StandardScaler],\n",
        "                 desc_cols: Optional[List[str]]) -> Tuple[DataLoader, DataLoader, Optional[np.ndarray]]:\n",
        "    # Split\n",
        "    train_df = df[df[\"Training/Test\"].str.strip().str.lower() == \"training\"].reset_index(drop=True)\n",
        "    test_df  = df[df[\"Training/Test\"].str.strip().str.lower() == \"test\"].reset_index(drop=True)\n",
        "\n",
        "    # Descriptors 1(fit scaler1 on training only)\n",
        "    train_desc = test_desc = None\n",
        "    if desc_cols:\n",
        "        if scaler is None:\n",
        "            scaler = StandardScaler().fit(train_df[desc_cols].to_numpy(dtype=np.float32))\n",
        "        train_desc = scaler.transform(train_df[desc_cols].to_numpy(dtype=np.float32))\n",
        "        test_desc  = scaler.transform(test_df[desc_cols].to_numpy(dtype=np.float32))\n",
        "\n",
        "\n",
        "    train_ds = SmiDescDataset(train_df[\"SMILES\"].tolist(),\n",
        "                              train_df[target_col].to_numpy(dtype=np.float32),\n",
        "                              tokenizer, cfg.max_length, train_desc)\n",
        "    test_ds  = SmiDescDataset(test_df[\"SMILES\"].tolist(),\n",
        "                              test_df[target_col].to_numpy(dtype=np.float32),\n",
        "                              tokenizer, cfg.max_length, test_desc)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True,\n",
        "                              collate_fn=collate_stack)\n",
        "    test_loader  = DataLoader(test_ds, batch_size=cfg.batch_size, shuffle=False,\n",
        "                              collate_fn=collate_stack)\n",
        "    return train_loader, test_loader, scaler.mean_ if desc_cols else None\n",
        "\n",
        "\n",
        "def setup_optimizer_scheduler(model, train_dataloader, epochs, lr=2e-5, lr_head=1e-3, warmup_ratio=0.1):\n",
        "\n",
        "    # separate ChemBERTa backbone vs fusion head params\n",
        "    backbone_params, head_params = [], []\n",
        "    for n, p in model.named_parameters():\n",
        "        if not p.requires_grad:\n",
        "            continue\n",
        "        if \"backbone\" in n:\n",
        "            backbone_params.append(p)\n",
        "        else:\n",
        "            head_params.append(p)\n",
        "    optimizer = torch.optim.AdamW([ {\"params\": backbone_params, \"lr\": lr},\n",
        "                                   {\"params\": head_params, \"lr\": lr_head}, ],\n",
        "                                  weight_decay=0.01)\n",
        "\n",
        "    total_steps = len(train_dataloader) * epochs\n",
        "    warmup_steps = int(total_steps * warmup_ratio)\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n",
        "\n",
        "    return optimizer, scheduler\n",
        "\n",
        "\n",
        "def train_model(model, train_loader, val_loader, optimizer, scheduler,\n",
        "                device, epochs=10, grad_clip=1.0, lambda_aux=0.2, lambda_div=0.05,\n",
        "                save_path=\"best_model.pt\", log_path=\"training_log.csv\"):\n",
        "    \"\"\"\n",
        "    Full trainer loop for ChemBERTaFusion with richer CSV logging.\n",
        "    \"\"\"\n",
        "    model.to(device)\n",
        "    best_val = float(\"inf\")\n",
        "\n",
        "    # --- Prepare CSV log ---\n",
        "    log_file = Path(log_path)\n",
        "    write_header = not log_file.exists()\n",
        "\n",
        "    with open(log_file, mode=\"a\", newline=\"\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        if write_header:\n",
        "            writer.writerow([\n",
        "                \"epoch\", \"train_loss\", \"val_loss\",\n",
        "                \"train_loss_cls\", \"train_loss_desc\",\n",
        "                \"val_loss_cls\", \"val_loss_desc\",\n",
        "                \"cos_sim\", \"gate_mean\", \"val_mae\"\n",
        "            ])\n",
        "\n",
        "    for epoch in range(1, epochs+1):\n",
        "        # ---- TRAIN ----\n",
        "        model.train()\n",
        "        train_loss, n_train = 0.0, 0\n",
        "        train_loss_cls, train_loss_desc = 0.0, 0.0\n",
        "        diag_accum = {\"cos_sim\": 0.0, \"gate_mean\": 0.0}\n",
        "\n",
        "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch} [Train]\")\n",
        "        for batch in pbar:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            descriptors = batch[\"descriptors\"].to(device)\n",
        "            targets = batch[\"labels\"].to(device)\n",
        "\n",
        "            preds, loss, diag = model(\n",
        "                input_ids, attention_mask, descriptors,\n",
        "                targets, lambda_aux=lambda_aux, lambda_div=lambda_div\n",
        "            )\n",
        "\n",
        "            loss.backward()\n",
        "            if grad_clip:\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            bs = input_ids.size(0)\n",
        "            train_loss += loss.item() * bs\n",
        "            n_train += bs\n",
        "\n",
        "            train_loss_cls += diag[\"loss_cls\"] * bs\n",
        "            train_loss_desc += diag[\"loss_desc\"] * bs\n",
        "            diag_accum[\"cos_sim\"] += diag[\"cos_sim\"] * bs\n",
        "            diag_accum[\"gate_mean\"] += diag[\"gate_mean\"] * bs\n",
        "\n",
        "            pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
        "\n",
        "        train_loss /= n_train\n",
        "        train_loss_cls /= n_train\n",
        "        train_loss_desc /= n_train\n",
        "        diag_accum = {k: v / n_train for k, v in diag_accum.items()}\n",
        "\n",
        "        # ---- VALIDATION ----\n",
        "        model.eval()\n",
        "        val_loss, n_val = 0.0, 0\n",
        "        val_loss_cls, val_loss_desc, val_mae = 0.0, 0.0, 0.0\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(val_loader, desc=f\"Epoch {epoch} [Val]\"):\n",
        "                input_ids = batch[\"input_ids\"].to(device)\n",
        "                attention_mask = batch[\"attention_mask\"].to(device)\n",
        "                descriptors = batch[\"descriptors\"].to(device)\n",
        "                targets = batch[\"labels\"].to(device)\n",
        "\n",
        "                preds, loss, diag = model(\n",
        "                    input_ids, attention_mask, descriptors,\n",
        "                    targets, lambda_aux=lambda_aux, lambda_div=lambda_div\n",
        "                )\n",
        "\n",
        "                bs = input_ids.size(0)\n",
        "                val_loss += loss.item() * bs\n",
        "                val_loss_cls += diag[\"loss_cls\"] * bs\n",
        "                val_loss_desc += diag[\"loss_desc\"] * bs\n",
        "                # MAE on predictions\n",
        "                val_mae += F.l1_loss(preds, targets, reduction=\"sum\").item()\n",
        "                n_val += bs\n",
        "\n",
        "        val_loss /= n_val\n",
        "        val_loss_cls /= n_val\n",
        "        val_loss_desc /= n_val\n",
        "        val_mae /= n_val\n",
        "\n",
        "        print(\n",
        "            f\"Epoch {epoch}: \"\n",
        "            f\"train_loss={train_loss:.4f}, val_loss={val_loss:.4f}, \"\n",
        "            f\"train_cls={train_loss_cls:.4f}, val_cls={val_loss_cls:.4f}, \"\n",
        "            f\"train_desc={train_loss_desc:.4f}, val_desc={val_loss_desc:.4f}, \"\n",
        "            f\"cos_sim={diag_accum['cos_sim']:.3f}, gate_mean={diag_accum['gate_mean']:.3f}, \"\n",
        "            f\"val_mae={val_mae:.4f}\"\n",
        "        )\n",
        "\n",
        "        # ---- Log to CSV ----\n",
        "        with open(log_file, mode=\"a\", newline=\"\") as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow([\n",
        "                epoch,\n",
        "                train_loss, val_loss,\n",
        "                train_loss_cls, train_loss_desc,\n",
        "                val_loss_cls, val_loss_desc,\n",
        "                diag_accum[\"cos_sim\"], diag_accum[\"gate_mean\"],\n",
        "                val_mae\n",
        "            ])\n",
        "\n",
        "        # ---- Save best ----\n",
        "        if val_loss < best_val:\n",
        "            best_val = val_loss\n",
        "            torch.save(model.state_dict(), save_path)\n",
        "            print(f\"✅ Saved best model (val_loss={val_loss:.4f})\")\n",
        "\n",
        "    print(\"Training complete.\")\n",
        "    model.load_state_dict(torch.load(save_path))\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "def predict(model, test_loader, device, return_numpy=True):\n",
        "    \"\"\"\n",
        "    Run inference on a test set.\n",
        "\n",
        "    Args:\n",
        "      model: trained ChemBERTaFusionV2\n",
        "      test_loader: DataLoader\n",
        "      device: torch.device\n",
        "      return_numpy: if True, returns numpy array\n",
        "\n",
        "    Returns:\n",
        "      preds: [N] predictions (torch.Tensor or np.ndarray)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "    all_preds = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            descriptors = batch[\"descriptors\"].to(device)\n",
        "\n",
        "            preds, _, _ = model(input_ids, attention_mask, descriptors)\n",
        "            all_preds.append(preds.cpu())\n",
        "\n",
        "    preds = torch.cat(all_preds, dim=0)\n",
        "\n",
        "    if return_numpy:\n",
        "        return preds.numpy()\n",
        "    return preds\n",
        "\n",
        "\n",
        "def train_gated_for_prop_desc(prop: str,\n",
        "                          desc: str,\n",
        "                          cfg: Config) -> Dict[str, Any]:\n",
        "    set_seed(cfg.seed)\n",
        "    ensure_dir(cfg.output_dir)\n",
        "\n",
        "    # ---- Load data\n",
        "    target_col = f\"{prop}-Measured\"\n",
        "    data_file = f\"Descriptors Data/{prop}_{desc}.parquet\"\n",
        "    sheet_name = f\"{prop}_{desc}\"\n",
        "    try:\n",
        "        df = pd.read_parquet(data_file)\n",
        "        desc_cols = df.columns[9:].to_list()\n",
        "    except:\n",
        "        raise ValueError(f\"{data_file} is not found.\")\n",
        "\n",
        "\n",
        "\n",
        "    # ---- Tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(cfg.model_name, use_fast=True)\n",
        "\n",
        "    scaler = None\n",
        "    if desc_cols:\n",
        "        scaler = StandardScaler()\n",
        "\n",
        "\n",
        "    # ---- Data loaders\n",
        "    train_loader, test_loader, _ = make_loaders(df, target_col, tokenizer, cfg, None, desc_cols)\n",
        "\n",
        "    # Fit scaler explicitly (make_loaders will fit inside if None, but we also want to save it)\n",
        "    if desc_cols:\n",
        "        train_df = df[df[\"Training/Test\"].str.strip().str.lower() == \"training\"].reset_index(drop=True)\n",
        "        scaler = StandardScaler().fit(train_df[desc_cols].to_numpy(dtype=np.float32))\n",
        "        scaler_path = os.path.join(cfg.output_dir, f\"{sheet_name.replace(' ','_')}_scaler.pkl\")\n",
        "        # joblib is heavier; simple numpy save up_for Colab friendliness\n",
        "        np.save(scaler_path.replace(\".pkl\", \"_mean.npy\"), scaler.mean_.astype(np.float32))\n",
        "        np.save(scaler_path.replace(\".pkl\", \"_scale.npy\"), scaler.scale_.astype(np.float32))\n",
        "        with open(scaler_path.replace(\".pkl\", \"_cols.json\"), \"w\") as f:\n",
        "            json.dump(desc_cols, f)\n",
        "        print(f\"Saved descriptor scaler → {scaler_path} (+mean/scale/cols files) for the descriptor.\")\n",
        "\n",
        "    # ---- Model\n",
        "    n_desc = len(desc_cols) if desc_cols else 0\n",
        "\n",
        "    model = ChemBERTaFusion(cfg.model_name, n_desc=n_desc, proj_dim=cfg.proj_dim,\n",
        "                              hidden_fuse=cfg.hidden_fuse, dropout=cfg.dropout,\n",
        "                              train_layers=cfg.train_layers, gate_temp=cfg.gate_temp,\n",
        "                             p_moddrop=cfg.p_moddrop).to(cfg.device)\n",
        "\n",
        "    # Parameter groups (smaller LR for backbone; larger for fusion head/regressor)\n",
        "    optimizer, scheduler = setup_optimizer_scheduler(model, train_loader, cfg.epochs,\n",
        "                                                    cfg.lr_backbone, cfg.lr_heads,\n",
        "                                                    cfg.warmup_ratio)\n",
        "\n",
        "    save_model = os.path.join(cfg.output_dir, cfg.save_path)\n",
        "    save_log = os.path.join(cfg.output_dir, f\"{prop}_{desc}_{cfg.log_path}\")\n",
        "    model = train_model(model, train_loader, test_loader, optimizer, scheduler, cfg.device,\n",
        "               cfg.epochs, cfg.grad_clip, cfg.lambda_aux, cfg.lambda_div, save_model, save_log)\n",
        "\n",
        "    # Predict on all rows (Training + Test)\n",
        "    # Prepare descriptors with saved scaler if available\n",
        "    if desc_cols:\n",
        "        mean = np.load(os.path.join(cfg.output_dir, f\"{sheet_name.replace(' ','_')}_scaler_mean.npy\"))\n",
        "        scale = np.load(os.path.join(cfg.output_dir, f\"{sheet_name.replace(' ','_')}_scaler_scale.npy\"))\n",
        "        with open(os.path.join(cfg.output_dir, f\"{sheet_name.replace(' ','_')}_scaler_cols.json\")) as f:\n",
        "            saved_cols = json.load(f)\n",
        "        # ensure consistent column order; fill missing with 0\n",
        "        X = df.reindex(columns=saved_cols)\n",
        "        X = X.to_numpy(dtype=np.float32)\n",
        "        X = (X - mean) / np.where(scale == 0, 1.0, scale)\n",
        "        desc = X\n",
        "    else:\n",
        "        desc = None\n",
        "\n",
        "    tokenizer_fast = tokenizer  # reuse\n",
        "    all_ds = SmiDescDataset(df[\"SMILES\"].tolist(),\n",
        "                            df[target_col].to_numpy(dtype=np.float32),\n",
        "                            tokenizer_fast, cfg.max_length, desc)\n",
        "    all_loader = DataLoader(all_ds, batch_size=cfg.batch_size, shuffle=False,\n",
        "                            collate_fn=collate_stack)\n",
        "\n",
        "    all_preds = predict(model, all_loader, cfg.device, cfg.return_numpy)\n",
        "\n",
        "    # Build results DF\n",
        "    new_results = pd.DataFrame({\n",
        "        \"Name\": df[\"NAME\"] if \"NAME\" in df.columns else pd.Series([None]*len(df)),\n",
        "        \"SMILES\": df[\"SMILES\"],\n",
        "        \"Observed\": df[target_col],\n",
        "        \"Predicted\": all_preds,\n",
        "        \"Training/Test\": df[\"Training/Test\"],\n",
        "    })\n",
        "\n",
        "    # Final metrics on Test only\n",
        "    obs_test = new_results[new_results[\"Training/Test\"].str.lower() == \"test\"][\"Observed\"].values\n",
        "    pred_test = new_results[new_results[\"Training/Test\"].str.lower() == \"test\"][\"Predicted\"].values\n",
        "    mae_v = mean_absolute_error(obs_test, pred_test)\n",
        "    rmse_v = rmse(obs_test, pred_test)\n",
        "    r2_v = r2_score(obs_test, pred_test)\n",
        "    print(f\"Final (best) Test metrics for {sheet_name} → MAE: {mae_v:.4f} | RMSE: {rmse_v:.4f} | R²: {r2_v:.4f}\")\n",
        "\n",
        "    # Save predictions parquet\n",
        "    pred_path = os.path.join(cfg.output_dir, f\"chemberta_{sheet_name.replace(' ','_')}.parquet\")\n",
        "    new_results.to_parquet(pred_path, index=False)\n",
        "    print(f\"Saved predictions → {pred_path}\")\n",
        "\n",
        "    return {\n",
        "        \"sheet\": sheet_name,\n",
        "        \"target_col\": target_col,\n",
        "        \"best_path\": save_model,\n",
        "        \"pred_path\": pred_path,\n",
        "        # \"corr_report\": corr_report_path,\n",
        "        \"MAE\": mae_v, \"RMSE\": rmse_v, \"R2\": r2_v,\n",
        "    }\n",
        "\n",
        "# ----------------------------\n",
        "# Predict helper (load & score new data)\n",
        "# ----------------------------\n",
        "def load_scaler_arrays(out_dir: str, sheet_name: str, desc_num: int):\n",
        "    prefix = os.path.join(out_dir, f\"{sheet_name.replace(' ','_')}_scaler\")\n",
        "    mean = np.load(prefix + f\"_mean{desc_num}.npy\")\n",
        "    scale = np.load(prefix + f\"_scale{desc_num}.npy\")\n",
        "    with open(prefix + f\"_cols{desc_num}.json\") as f:\n",
        "        cols = json.load(f)\n",
        "    return mean, scale, cols\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Multi-property runner\n",
        "# ----------------------------\n",
        "def run_all_properties_descriptors(prop_names: str, desc_names: List[str], cfg: Config):\n",
        "    ensure_dir(cfg.output_dir)\n",
        "    perf_rows = []\n",
        "    for prop in prop_names:\n",
        "        for desc in desc_names:\n",
        "            print(f\"\\n=== Processing the file: {prop}_{desc} ===\")\n",
        "            result = train_gated_for_prop_desc(prop, desc, cfg)\n",
        "            perf_rows.append([f\"{prop}_{desc}\", result[\"MAE\"], result[\"RMSE\"], result[\"R2\"]])\n",
        "    perf_df = pd.DataFrame(perf_rows, columns=[\"Property\", \"MAE\", \"RMSE\", \"R2\"])\n",
        "    stats_path = os.path.join(cfg.output_dir, f\"chemberta_FiLM_Fusion_stats.csv\")\n",
        "    perf_df.to_csv(stats_path, index=False)\n",
        "    print(f\"\\n📊 All-property stats saved → {stats_path}\")\n",
        "    return perf_df\n"
      ],
      "metadata": {
        "id": "Jtx0g5fSzKnZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cfg = Config(\n",
        "    output_dir=\"chemberta_gated_results\",\n",
        "    epochs=30,\n",
        "    batch_size=8,\n",
        "    max_length=128,\n",
        "    proj_dim=128,\n",
        "    # train_layers=3,      # unfreeze last 2 transformer blocks\n",
        "    lr_backbone=1e-5,\n",
        "    lr_heads=1e-5\n",
        ")\n",
        "\n",
        "prop_names = [\"Log VP\", \"MP\", \"BP\", \"LogBCF\", \"LogS\", \"LogP\"]\n",
        "# prop_names = [\"LogP\"]\n",
        "desc_names = [\"MACCS\", \"Morgan\", \"pwav\"]\n",
        "\n",
        "perf_df = run_all_properties_descriptors(prop_names, desc_names, cfg)\n",
        "perf_df"
      ],
      "metadata": {
        "id": "qClaZZCzzhmu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pgirnuwPzvdJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
