{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPY+gdDL6bglqKrLmsLKEC2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MZiaAfzal71/Average_Weighted_Path_Vector/blob/main/Data%20Files/Data%20Statistics/XGB_SHAP_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rj7USXvWJER8"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/MZiaAfzal71/Average_Weighted_Path_Vector.git\n",
        "%cd Average_Weighted_Path_Vector/Data\\ Files"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install osfclient\n",
        "import shutil\n",
        "from osfclient.api import OSF\n",
        "from subprocess import run\n",
        "import os\n",
        "\n",
        "# Replace with your OSF project ID\n",
        "project_id = \"p5ga2\"   # e.g. from https://osf.io/abcd3/\n",
        "osf = OSF()\n",
        "project = osf.project(project_id)\n",
        "store = project.storage(\"osfstorage\")\n",
        "\n",
        "desc_folder = []\n",
        "for fold in store.folders:\n",
        "    if fold.path.strip(\"/\") == \"Descriptors Data\":\n",
        "        desc_folder.append(fold)\n",
        "        break\n",
        "\n",
        "\n",
        "# Download all files and keep folder structure\n",
        "for folder in desc_folder:\n",
        "  for f in folder.files:\n",
        "      local_path = f.path.strip(\"/\")            # keep folders\n",
        "      local_dir = os.path.dirname(local_path)   # extract dir\n",
        "      if local_dir and not os.path.exists(local_dir):\n",
        "          os.makedirs(local_dir, exist_ok=True) # create dirs if missing\n",
        "      with open(local_path, \"wb\") as out:\n",
        "          f.write_to(out)\n",
        "      if local_path.endswith(\".zip\"):\n",
        "        command = f\"unzip '{local_path}' -d '{local_dir}'\"\n",
        "        run(command, shell=True)\n",
        "        print(f\"\\nUnzipped {local_path} -> {local_dir}\")"
      ],
      "metadata": {
        "id": "NjdcHXQDJMzI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import r2_score, mean_absolute_error, root_mean_squared_error\n",
        "\n",
        "# === Paths & Settings ===\n",
        "input_dir = \"Descriptors Data\"\n",
        "output_dir = \"XGBoost Results\"\n",
        "shap_dir = os.path.join(output_dir, \"SHAP_Analysis\")\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "os.makedirs(shap_dir, exist_ok=True)\n",
        "\n",
        "property_sheets = [\"Log VP\", \"MP\", \"BP\", \"LogBCF\", \"LogS\", \"LogP\"]\n",
        "\n",
        "# === Main Loop ===\n",
        "for prop in property_sheets:\n",
        "    # Input file\n",
        "    input_file = os.path.join(input_dir, f\"{prop}_pwav.parquet\")\n",
        "    print(f\"\\nðŸ”¹ Processing: {input_file}\")\n",
        "\n",
        "    # Read data\n",
        "    df = pd.read_parquet(input_file)\n",
        "    prop_pred = f\"{prop}-Measured\"\n",
        "\n",
        "    # Feature matrix & target\n",
        "    X = df.iloc[:, 9:]\n",
        "    y = df[prop_pred]\n",
        "\n",
        "    # Train/test split\n",
        "    train_idx = df[df[\"Training/Test\"] == \"Training\"].index.tolist()\n",
        "    test_idx  = df[df[\"Training/Test\"] == \"Test\"].index.tolist()\n",
        "\n",
        "    X_train, X_valid = X.iloc[train_idx], X.iloc[test_idx]\n",
        "    y_train, y_valid = y.iloc[train_idx], y.iloc[test_idx]\n",
        "\n",
        "    # Define model\n",
        "    model = XGBRegressor(random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # === SHAP Analysis ===\n",
        "    print(\"ðŸ” Running SHAP analysis...\")\n",
        "    explainer = shap.TreeExplainer(model)\n",
        "    shap_values = explainer(X_valid)\n",
        "\n",
        "    # Summary plot (bar chart: mean |SHAP| per feature)\n",
        "    shap.summary_plot(shap_values, X_valid, plot_type=\"bar\", show=False)\n",
        "    plt.title(f\"SHAP Feature Importance - {prop} pwav\")\n",
        "    plt.tight_layout()\n",
        "    shap_file_bar = os.path.join(shap_dir, f\"{prop}_pwav_shap_bar.png\")\n",
        "    plt.savefig(shap_file_bar, dpi=300)\n",
        "    plt.close()\n",
        "\n",
        "    # Summary plot (beeswarm: detailed per-sample effects)\n",
        "    shap.summary_plot(shap_values, X_valid, show=False)\n",
        "    plt.title(f\"SHAP Beeswarm - {prop} pwav\")\n",
        "    plt.tight_layout()\n",
        "    shap_file_bee = os.path.join(shap_dir, f\"{prop}_pwav_shap_beeswarm.png\")\n",
        "    plt.savefig(shap_file_bee, dpi=300)\n",
        "    plt.close()\n",
        "\n",
        "    print(f\"ðŸ“ˆ SHAP plots saved:\\n   âž¤ {shap_file_bar}\\n   âž¤ {shap_file_bee}\")\n",
        "\n",
        "print(\"\\nðŸŽ‰ All files processed & SHAP analysis completed!\")\n"
      ],
      "metadata": {
        "id": "e-sGyBSaJQjw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from xgboost import XGBRegressor\n",
        "from matplotlib.colors import PowerNorm\n",
        "\n",
        "# === Paths ===\n",
        "input_dir = \"Descriptors Data\"\n",
        "output_dir = \"XGBoost Results\"\n",
        "shap_dir = os.path.join(output_dir, \"SHAP_Analysis\")\n",
        "os.makedirs(shap_dir, exist_ok=True)\n",
        "\n",
        "property_sheets = [\"Log VP\", \"MP\", \"BP\", \"LogBCF\", \"LogS\", \"LogP\"]\n",
        "\n",
        "# === Helper: compute SHAP importances for one property ===\n",
        "def compute_shap_importances(prop, input_dir, topN=20):\n",
        "    \"\"\"Train XGBRegressor on PWAV descriptors for a property and return mean SHAP values + top features.\"\"\"\n",
        "    input_file = os.path.join(input_dir, f\"{prop}_pwav.parquet\")\n",
        "    df = pd.read_parquet(input_file)\n",
        "\n",
        "    X, y = df.iloc[:, 9:], df[f\"{prop}-Measured\"]\n",
        "\n",
        "    # Train/test split\n",
        "    train_idx = df[df[\"Training/Test\"] == \"Training\"].index\n",
        "    test_idx  = df[df[\"Training/Test\"] == \"Test\"].index\n",
        "    X_train, X_valid = X.loc[train_idx], X.loc[test_idx]\n",
        "    y_train = y.loc[train_idx]\n",
        "\n",
        "    # Train model\n",
        "    model = XGBRegressor(random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # SHAP values\n",
        "    explainer = shap.TreeExplainer(model)\n",
        "    shap_values = explainer(X_valid)\n",
        "\n",
        "    # Mean |SHAP| per feature\n",
        "    mean_shap = pd.Series(\n",
        "        shap_values.abs.mean(0).values,\n",
        "        index=X_valid.columns,\n",
        "        name=prop\n",
        "    )\n",
        "\n",
        "    # Top-N features\n",
        "    top_features = mean_shap.nlargest(topN)\n",
        "\n",
        "    return mean_shap, top_features\n",
        "\n",
        "# === Main analysis ===\n",
        "all_shap_importances = {}\n",
        "top_features_sets = {}\n",
        "heatmap_data = {}\n",
        "\n",
        "for prop in property_sheets:\n",
        "    print(f\"\\nðŸ”¹ Processing {prop}...\")\n",
        "    mean_shap, top_features = compute_shap_importances(prop, input_dir, topN=20)\n",
        "    all_shap_importances[prop] = mean_shap\n",
        "    heatmap_data[prop] = top_features\n",
        "    top_features_sets[prop] = set(top_features.index)\n",
        "\n",
        "# === Save combined SHAP importances ===\n",
        "shap_df = pd.concat(all_shap_importances, axis=1)\n",
        "shap_df[\"Global_Avg\"] = shap_df.mean(axis=1)\n",
        "\n",
        "combined_file = os.path.join(shap_dir, \"PWAV_SHAP_Combined.xlsx\")\n",
        "shap_df.to_excel(combined_file)\n",
        "print(f\"\\nâœ… Combined SHAP importance saved to: {combined_file}\")\n",
        "\n",
        "# === Pairwise Jaccard similarity of top-N sets ===\n",
        "props = list(top_features_sets.keys())\n",
        "jaccard_matrix = pd.DataFrame(index=props, columns=props, dtype=float)\n",
        "\n",
        "for p1 in props:\n",
        "    for p2 in props:\n",
        "        inter = len({s.split(\"_\", 1)[1] for s in top_features_sets[p1]} &\n",
        "                    {s.split(\"_\", 1)[1] for s in top_features_sets[p2]})\n",
        "        union = len({s.split(\"_\", 1)[1] for s in top_features_sets[p1]} |\n",
        "                    {s.split(\"_\", 1)[1] for s in top_features_sets[p2]})\n",
        "        jaccard_matrix.loc[p1, p2] = inter / union\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(jaccard_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\")\n",
        "plt.title(\"Jaccard Similarity of Top-20 PWAV Features Across Properties\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(shap_dir, \"PWAV_SHAP_Jaccard_Heatmap.png\"), dpi=300)\n",
        "plt.close()\n",
        "print(f\"ðŸ“ˆ Jaccard heatmap saved: {os.path.join(shap_dir, 'PWAV_SHAP_Jaccard_Heatmap.png')}\")\n",
        "\n",
        "# === Annotated heatmaps (2 rows Ã— 3 columns) ===\n",
        "fig, axes = plt.subplots(2, 3, figsize=(22, 18), sharey=True)\n",
        "axes = axes.flatten()\n",
        "\n",
        "# Shared color scaling\n",
        "all_vals = pd.concat(heatmap_data.values())\n",
        "vmin, vmax = all_vals.min(), all_vals.max()\n",
        "\n",
        "for ax, prop in zip(axes, property_sheets):\n",
        "    values = heatmap_data[prop].to_frame()\n",
        "\n",
        "    sns.heatmap(\n",
        "        values, cmap=\"tab20\", cbar=False,\n",
        "        vmin=vmin, vmax=vmax,\n",
        "        norm=PowerNorm(gamma=0.5),\n",
        "        linewidths=0.3, ax=ax\n",
        "    )\n",
        "\n",
        "    # Annotate each cell with feature = value\n",
        "    for y_idx, feature in enumerate(values.index):\n",
        "        shap_val = values.loc[feature, prop]\n",
        "        ax.text(\n",
        "            0.5, y_idx + 0.5,\n",
        "            f\"{feature} = {shap_val:.2f}\",\n",
        "            ha=\"center\", va=\"center\",\n",
        "            color=\"white\", fontsize=12, fontweight=\"bold\"\n",
        "        )\n",
        "\n",
        "    ax.set_title(prop, fontsize=16)\n",
        "    ax.set_axis_off()\n",
        "\n",
        "# Shared colorbar\n",
        "cbar_ax = fig.add_axes([0.92, 0.25, 0.015, 0.5])\n",
        "sm = plt.cm.ScalarMappable(cmap=\"tab20\",\n",
        "                           norm=PowerNorm(gamma=0.5, vmin=vmin, vmax=vmax))\n",
        "fig.colorbar(sm, cax=cbar_ax, label=\"Mean |SHAP|\")\n",
        "\n",
        "plt.suptitle(\"Top-20 PWAV Features per Property (Annotated)\", fontsize=22)\n",
        "fig.subplots_adjust(left=0.05, right=0.9, top=0.94, bottom=0.1,\n",
        "                    wspace=0.05, hspace=0.1)\n",
        "\n",
        "heatmap_file = os.path.join(shap_dir, \"PWAV_SHAP_Heatmaps_Annotated_2rows.png\")\n",
        "plt.savefig(heatmap_file, dpi=300)\n",
        "plt.close()\n",
        "print(f\"âœ… Annotated heatmaps saved: {heatmap_file}\")\n",
        "\n",
        "# === Contribution Breakdown ===\n",
        "combined_file = os.path.join(shap_dir, \"PWAV_SHAP_Combined.xlsx\")\n",
        "df_combined = pd.read_excel(combined_file)\n",
        "\n",
        "\n",
        "# For each property, compute % contribution of top-64 vs others\n",
        "contrib_data = {}\n",
        "for prop in property_sheets:\n",
        "    col = prop  # property column with SHAP importances\n",
        "    list_of_prop_vals = df_combined[prop].dropna().index.tolist()\n",
        "\n",
        "    top64_sum = df_combined.loc[list_of_prop_vals[:64], 'Global_Avg'].sum()\n",
        "    rest_sum = df_combined.loc[list_of_prop_vals[64:], 'Global_Avg'].sum()\n",
        "\n",
        "    total = top64_sum + rest_sum\n",
        "\n",
        "    contrib_data[prop] = {\n",
        "        \"Top64\": 100 * top64_sum / total,\n",
        "        \"Others\": 100 * rest_sum / total\n",
        "    }\n",
        "\n",
        "\n",
        "df_contrib = pd.DataFrame(contrib_data).T\n",
        "\n",
        "# --- Stacked bar plot ---\n",
        "ax = df_contrib.plot(\n",
        "    kind=\"bar\", stacked=True, figsize=(10, 6),\n",
        "    color=[\"#1f77b4\", \"#ff7f0e\"], edgecolor=\"black\"\n",
        ")\n",
        "plt.ylabel(\"Contribution (%)\")\n",
        "plt.title(\"PWAV SHAP Contributions: Top-64 vs Others\")\n",
        "plt.legend(title=\"Category\")\n",
        "# Add annotations inside bars\n",
        "for c_idx, col in enumerate(df_contrib.columns):\n",
        "    for i, val in enumerate(df_contrib[col]):\n",
        "        if val > 1:  # skip if contribution is tiny\n",
        "            # Compute bar position\n",
        "            bottom = df_contrib.iloc[i, :c_idx].sum()\n",
        "            ax.text(\n",
        "                i, bottom + val/2, f\"{val:.1f}%\",\n",
        "                ha=\"center\", va=\"center\", fontsize=10, color=\"white\", fontweight=\"bold\",\n",
        "                rotation=0\n",
        "            )\n",
        "plt.xticks(rotation=0)\n",
        "stacked_file = os.path.join(shap_dir, \"PWAV_SHAP_StackedBar.png\")\n",
        "plt.savefig(stacked_file, dpi=300, bbox_inches=\"tight\")\n",
        "plt.close()\n",
        "print(f\"ðŸ“ˆ SHAP Stacked Contributions: Top 64 vs Others is saved: {os.path.join(shap_dir, 'PWAV_SHAP_StackedBar.png')}\")\n"
      ],
      "metadata": {
        "id": "MUcEoPB4jcB_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5HSh6whhpimC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}