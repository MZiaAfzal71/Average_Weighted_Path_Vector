{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOFFnyGGlL75Y+QEn44BAwi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MZiaAfzal71/Average_Weighted_Path_Vector/blob/main/Data%20Files/Chemprop/Chemprop_wo_Descriptors.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ti4zCFS7Oopm"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/MZiaAfzal71/Average_Weighted_Path_Vector.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd Average_Weighted_Path_Vector/Data\\ Files"
      ],
      "metadata": {
        "id": "nf_KSlwyPF8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chemprop"
      ],
      "metadata": {
        "id": "9mIvmX0IPVO-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np # linear algebra\n",
        "\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "from pathlib import Path\n",
        "from sklearn.metrics import r2_score, mean_absolute_error, root_mean_squared_error\n",
        "\n",
        "import torch\n",
        "from lightning import pytorch as pl\n",
        "from lightning.pytorch.callbacks import ModelCheckpoint\n",
        "\n",
        "from chemprop import data, featurizers, models, nn #, uncertainty\n",
        "\n",
        "# from chemprop.models import save_model, load_model\n",
        "import sys\n",
        "import os\n",
        "# from chemprop.cli.predict import find_models"
      ],
      "metadata": {
        "id": "xL4lB9G2PGiF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_path =  \"Excel Files/Zang_Data.xlsx\" # path to your data .xlsx file\n",
        "\n",
        "# Path to extra molecule descriptors\n",
        "property = [\"Log VP\", \"MP\", \"BP\", \"LogBCF\", \"LogS\", \"LogP\"]"
      ],
      "metadata": {
        "id": "Y2Ig9bfLQGBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_files_dir = \"Results Chemprop/without descriptors\"\n",
        "os.makedirs(output_files_dir, exist_ok=True)\n",
        "\n",
        "perf_stats = []\n",
        "for prop in property:\n",
        "  out_dir = Path(f\"chemprop_model/ch_pt_{prop}\") # directory for storing the best model after training\n",
        "  os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "  smiles_column = 'SMILES' # name of the column containing SMILES strings\n",
        "  target_columns = [f'{prop}-Measured'] # list of names of the columns containing targets\n",
        "\n",
        "  df_input = pd.read_excel(input_path, sheet_name=prop)\n",
        "\n",
        "  smis = df_input.loc[:, smiles_column].values\n",
        "  ys = df_input.loc[:, target_columns].values\n",
        "\n",
        "  all_data = [data.MoleculeDatapoint.from_smi(sm, y) for sm, y in zip(smis, ys)]\n",
        "\n",
        "  # Get training and test indices\n",
        "  train_indices = [df_input[df_input[\"Training/Test\"] == \"Training\"].index.to_list()]\n",
        "  test_indices = df_input[df_input[\"Training/Test\"] == \"Test\"].index.to_list()\n",
        "\n",
        "  half = len(test_indices) // 2\n",
        "  val_indices = [test_indices[:half]]\n",
        "  final_test_indices = [test_indices[half:]]\n",
        "\n",
        "  train_data, val_data, test_data = data.split_data_by_indices(\n",
        "      all_data, train_indices, val_indices, final_test_indices\n",
        "  )\n",
        "\n",
        "  featurizer = featurizers.SimpleMoleculeMolGraphFeaturizer()\n",
        "\n",
        "  train_dset = data.MoleculeDataset(train_data[0], featurizer)\n",
        "  scaler = train_dset.normalize_targets()\n",
        "\n",
        "  val_dset = data.MoleculeDataset(val_data[0], featurizer)\n",
        "  val_dset.normalize_targets(scaler)\n",
        "\n",
        "  test_dset = data.MoleculeDataset(test_data[0], featurizer)\n",
        "\n",
        "  # Featurize the train and val datasets to save computation time.\n",
        "  train_dset.cache = True\n",
        "  val_dset.cache = True\n",
        "\n",
        "  train_loader = data.build_dataloader(train_dset)\n",
        "  val_loader = data.build_dataloader(val_dset, shuffle=False)\n",
        "  test_loader = data.build_dataloader(test_dset, shuffle=False)\n",
        "\n",
        "  mp = nn.BondMessagePassing()\n",
        "  agg = nn.NormAggregation()\n",
        "\n",
        "  ffn_input_dim = mp.output_dim\n",
        "  output_transform = nn.UnscaleTransform.from_standard_scaler(scaler)\n",
        "  ffn = nn.RegressionFFN(input_dim=ffn_input_dim, output_transform=output_transform)\n",
        "\n",
        "  metric_list = [nn.metrics.RMSE(), nn.metrics.MAE(), nn.metrics.R2Score()] # Only the first metric is used for training and early stopping\n",
        "\n",
        "  mpnn = models.MPNN(mp, agg, ffn, metrics=metric_list)\n",
        "\n",
        "  # Configure model checkpointing\n",
        "  check_pointing = ModelCheckpoint(\n",
        "      out_dir,  # Directory where model checkpoints will be saved\n",
        "      \"best-{epoch}-{val_loss:.3f}\",  # Filename format for checkpoints, including epoch and validation loss\n",
        "      \"val_loss\",  # Metric used to select the best checkpoint (based on validation loss)\n",
        "      mode=\"min\",  # Save the checkpoint with the lowest validation loss (minimization objective)\n",
        "      save_last=True,  # Always save the most recent checkpoint, even if it's not the best\n",
        "  )\n",
        "\n",
        "  trainer = pl.Trainer(\n",
        "      logger=False,\n",
        "      enable_checkpointing=True, # Use `True` if you want to save model checkpoints. The checkpoints will be saved in the `checkpoints` folder.\n",
        "      enable_progress_bar=True,\n",
        "      accelerator=\"auto\",\n",
        "      devices=1,\n",
        "      max_epochs=30, # number of epochs to train for\n",
        "      callbacks=[check_pointing], # Use the configured checkpoint callback\n",
        "  )\n",
        "\n",
        "  trainer.fit(mpnn, train_loader, val_loader)\n",
        "  best_model_path = check_pointing.best_model_path\n",
        "  trained_model = mpnn.__class__.load_from_checkpoint(best_model_path)\n",
        "\n",
        "  results = trainer.test(dataloaders=test_loader)\n",
        "\n",
        "  alldata_dset = data.MoleculeDataset(all_data, featurizer)\n",
        "  alldata_loader = data.build_dataloader(alldata_dset, shuffle=False)\n",
        "\n",
        "  with torch.inference_mode():\n",
        "      trainer = pl.Trainer(\n",
        "          logger=None,\n",
        "          enable_progress_bar=True,\n",
        "          accelerator=\"cpu\",\n",
        "          devices=1\n",
        "      )\n",
        "      alldata_preds = trainer.predict(trained_model, alldata_loader)\n",
        "\n",
        "  alldata_preds = np.concatenate(alldata_preds, axis=0)\n",
        "\n",
        "\n",
        "  # Construct result dataframe\n",
        "  new_results = pd.DataFrame({\n",
        "      'Name': df_input['NAME'],\n",
        "      'SMILES': df_input['SMILES'],\n",
        "      'Observed': df_input[target_columns[0]],\n",
        "      'Predicted': alldata_preds.ravel(),\n",
        "      'Training/Test': df_input['Training/Test']})\n",
        "\n",
        "  # Report performance\n",
        "  obs_test = new_results[new_results['Training/Test'] == 'Test']['Observed'].values\n",
        "  pred_test = new_results[new_results['Training/Test'] == 'Test']['Predicted'].values\n",
        "\n",
        "  mae_test = mean_absolute_error(obs_test, pred_test)\n",
        "  rmse_test = root_mean_squared_error(obs_test, pred_test)\n",
        "  r2_test = r2_score(obs_test, pred_test)\n",
        "\n",
        "  perf_stats.append([prop, mae_test, rmse_test, r2_test])\n",
        "\n",
        "  print(f'üß¨ Processing descriptor: D-MPNN {prop}')\n",
        "  print(f\"üîç MAE:  {mae_test:.2f}\")\n",
        "  print(f\"üîç RMSE: {rmse_test:.2f}\")\n",
        "  print(f\"üîç R¬≤:   {r2_test:.3f}\")\n",
        "\n",
        "  new_results.to_parquet(f'{output_files_dir}/chemprop_{prop}.parquet', index=False)\n",
        "\n",
        "  print(f'The file chemprop_{prop}.parquet is saved to the directory {output_files_dir}!')\n",
        "\n",
        "perf_stats_df = pd.DataFrame(perf_stats, columns=[\"Property\", \"MAE\", \"RMSE\", \"R2\"])\n",
        "perf_stats_df.to_csv(f'{output_files_dir}/chemprop_stats.csv', index=False)\n",
        "print(f'The Chemprop stats on test data for all properties has been saved to chemprop_stats.csv!')"
      ],
      "metadata": {
        "id": "muyy-0lIQ_UV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}